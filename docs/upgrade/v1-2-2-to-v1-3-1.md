---
sidebar_position: 2
sidebar_label: Upgrade from v1.2.2/v1.3.0 to v1.3.1
title: "Upgrade from v1.2.2/v1.3.0 to v1.3.1"
---

<head>
  <link rel="canonical" href="https://docs.harvesterhci.io/v1.3/upgrade/v1-2-2-to-v1-3-1"/>
</head>

## General information

An **Upgrade** button appears on the **Dashboard** screen whenever a new Harvester version that you can upgrade to becomes available. For more information, see [Start an upgrade](./automatic.md#start-an-upgrade).

For air-gapped environments, see [Prepare an air-gapped upgrade](./automatic.md#prepare-an-air-gapped-upgrade).


## Known issues

---

### 1. Upgrade stuck after the first node upgraded

While upgrading a Harvester cluster from v1.2.2 or v1.3.0 to v1.3.1, you see the upgrade process is stuck after the first node is upgraded. For example:

![](/img/v1.3/upgrade/known_issues/6041-stuck-on-first-node.png)

To resolve this issue, perform the following steps:

1. Identify the cluster status:

    ```
    kubectl get clusters.provisioning.cattle.io local -n fleet-local -o yaml
    ```

    Example output:
    ```
    ...
      - lastUpdateTime: "2024-06-18T23:37:39Z"
        message: 'configuring bootstrap node(s) custom-9cb22ccf7984: waiting for kubelet
          to update'
        reason: Waiting
        status: Unknown
        type: Updated
      - lastUpdateTime: "2024-06-18T23:37:39Z"
        message: 'configuring bootstrap node(s) custom-9cb22ccf7984: waiting for kubelet
          to update'
        reason: Waiting
        status: Unknown
        type: Provisioned
    ```

    If you see the `waiting for kubelet` message, continue to the next step.

1. Check the `capi-controller-manager` pod's logs:

    ```
    kubectl logs -n cattle-provisioning-capi-system deployment/capi-controller-manager
    ```

    If the output is similar to the following example, the issue likely exists in the cluster.
    ```
    2024-06-19T08:54:22.407423986Z W0619 08:54:22.407257       1 reflector.go:424] k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: failed to list *v1.Node: Unauthorized
    2024-06-19T08:54:22.407470069Z E0619 08:54:22.407283       1 reflector.go:140] k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: Failed to watch *v1.Node: failed to list *v1.Node: Unauthorized
    2024-06-19T08:55:05.153396619Z W0619 08:55:05.153190       1 reflector.go:424] k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: failed to list *v1.Node: Unauthorized
    2024-06-19T08:55:05.153438978Z E0619 08:55:05.153217       1 reflector.go:140] k8s.io/client-go@v0.26.1/tools/cache/reflector.go:169: Failed to watch *v1.Node: failed to list *v1.Node: Unauthorized
    ```

1. Apply the following workaround to resume the upgrade:

    Get the `capi-controller-manager` pod name, for example:
    ```
    $ kubectl get pods -n cattle-provisioning-capi-system | grep capi-controller-manager-
    capi-controller-manager-6f497d5574-k6b5q   1/1     Running   2 (12m ago)   76d
    ```

    Kill and restart the `capi-controller-manager` pod:
    ```
    $ kubectl delete -n cattle-provisioning-capi-system pods/capi-controller-manager-6f497d5574-k6b5q
    pod "capi-controller-manager-6f497d5574-k6b5q" deleted
    ```


- Related issue:
  - [[BUG] Upgrade v1.2.2->v1.3.1 stuck at Waiting for kubelet to update, even the node has been updated](https://github.com/harvester/harvester/issues/6041)

---
