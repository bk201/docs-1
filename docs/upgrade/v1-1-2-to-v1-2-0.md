---
sidebar_position: 2
sidebar_label: Upgrade from v1.1.2 to v1.2.0
title: "Upgrade from v1.1.2 to v1.2.0"
---

<head>
  <link rel="canonical" href="https://docs.harvesterhci.io/v1.2/upgrade/v1-1-2-to-v1-2-0"/>
</head>

:::danger

Please do not upgrade a running cluster to v1.2.0 if your machine has an **Intel E810** NIC card. We saw some reports that the NIC card has a problem when added to a bonding device. Please check this issue for more info: https://github.com/harvester/harvester/issues/3860.

:::

## General information

:::tip

Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this [URL](https://github.com/harvester/upgrade-helpers/tree/main/pre-check/v1.1.x) for the script.
:::

Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to [start an upgrade](./automatic.md#start-an-upgrade).

For the air-gap env upgrade, please refer to [prepare an air-gapped upgrade](./automatic.md#prepare-an-air-gapped-upgrade).


## Known Issues

---

### 1. The upgrade can't start because of `"validator.harvesterhci.io" denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready`

If a cluster is configured with a **storage network**, an upgrade can't start with the following message.

![](/img/v1.2/upgrade/known_issues/3839-error.png)

- Related issue
  - [[Doc] upgrade stuck while upgrading system service with alertmanager and prometheus](https://github.com/harvester/harvester/issues/3839)
- Workaround
  - https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192

---

### 2. An upgrade is stuck on creating the upgrade repository

An upgrade is pending while creating upgrade the repository:

  ![](/img/v1.2/upgrade/known_issues/4246-pending.png)

Please do the following operations to check if the cluster runs into the issue:

1. Check the upgrade repository pod:

    ![](/img/v1.2/upgrade/known_issues/4246-upgrade-repo-pod.png)

    The `virt-launcher-upgrade-repo-hvst-<upgrade-name>` pod stays in `ContainerCreating`.

2. Check the upgrade repository volume in the Longhorn GUI.

     1. [Go to Longhorn GUI](../troubleshooting/harvester.md#access-embedded-rancher-and-longhorn-dashboards).
     2. Navigate to the **Volume** page.
     3. Check the upgrade repository VM volume. It should be attached to a pod called `virt-launcher-upgrade-repo-hvst-<upgrade-name>`. If one of the volume's replicas stays in `Stopped` (gray color), the cluster is running into the issue.

      ![](/img/v1.2/upgrade/known_issues/4246-pending-replica.png)

- Related issue:
  - [[BUG] upgrade stuck on create upgrade VM](https://github.com/harvester/harvester/issues/4246)
- Workaround:
  - Delete the `Stopped` replica from Longhorn GUI. Or,
  - [Start over the upgrade](./troubleshooting.md#start-over-an-upgrade).

---

### 3. An upgrade is stuck when pre-draining a node

Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count >= 3) before upgrading a node. Generally, you can check volumes' health if an upgrade is stuck in the "pre-draining" state.

Visit ["Access Embedded Longhorn"](../troubleshooting/harvester.md#access-embedded-rancher-and-longhorn-dashboards) to see how to access the embedded Longhorn GUI.

You can also check the pre-drain job logs. Please refer to [Phase 4: Upgrade nodes](./troubleshooting.md#phase-4-upgrade-nodes) in the troubleshooting guide.

---

### 4. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline

An upgrade fails, as shown in the screenshot below:

![](/img/v1.2/upgrade/known_issues/2894-deadline.png)


- Related issue:
  - [[BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline](https://github.com/harvester/harvester/issues/2894)
- Workaround:
  - https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690


---

### 5. An upgrade is stuck in the Pre-drained state

You might see an upgrade is stuck in the "pre-drained" state:

![](/img/v1.2/upgrade/known_issues/3730-stuck.png)

This could be caused by a misconfigured PDB. To check if that's the case, perform the following steps:

1. Assume the stuck node is `harvester-node-1`.
1. Check the `instance-manager-e` or `instance-manager-r` pod names on the stuck node:

    ```
    $ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager
    instance-manager-r-d4ed2788          1/1     Running   0              3d8h
    ```

    The output above shows that the `instance-manager-r-d4ed2788` pod is on the node. 

1. Check Rancher logs and verify that the `instance-manager-e` or `instance-manager-r` pod can't be drained:

    ```
    $ kubectl logs deployment/rancher -n cattle-system
    ...
    2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def
    2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788
    2023-03-28T17:10:55.080933607Z error when evicting pods/"instance-manager-r-d4ed2788" -n "longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
    ```

1. Run the command to check if there is a PDB associated with the stuck node:

    ```
    $ kubectl get pdb -n longhorn-system -o yaml | yq '.items[] | select(.spec.selector.matchLabels."longhorn.io/node"=="harvester-node-1") | .metadata.name'
    instance-manager-r-466e3c7f
    ```

1. Check the owner of the instance manager to this PDB:

    ```
    $ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID'
    harvester-node-2
    ```

    If the output doesn't match the stuck node (in this example output, `harvester-node-2` doesn't match the stuck node `harvester-node-1`), then we can conclude this issue happens.

1. Before applying the workaround, check if all volumes are healthy:

    ```
    kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == "attached")| .status.robustness'
    ```

    The output should all be `healthy`. If this is not the case, you might want to uncordon nodes to make the volume healthy again.

1.  Remove the misconfigured PDB:

    ```
    kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system
    ```

- Related issue:
  - [[BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0->v1.1.2-rc4](https://github.com/harvester/harvester/issues/3730 )

---
